{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3a02adc9e884466bc8c79db549cc3d2",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true,
       "underline": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ]
   },
   "source": [
    "# Title of notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4546bee655b14a5dbf393161f1228e60",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Brief 1-2 sentence description of notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment\n",
    "    - Observing fighting behavior between mice in their home cage in different settings. Mice are categorized as winner and loser based on if they initiated the behavior or if they are recieving it. Recording sessions are usually from 30 minutes to a hour with cages of 2 to 6 mice.\n",
    "- Data\n",
    "    - Excel spreadsheet of recorded fighting behavior. The relevant columns are those of the \"Date\" of the recording, the \"winner\" of the interaction, and the \"loser\" of the interaction. Each row will be for one interaction between two mice.\n",
    "    - There is a cage for each sheet of the spreadsheet\n",
    "    - For each recording session, we will assume that the date will be specified for that first row. We will use this to seperate all the rows into seperate sessions.\n",
    "- Purpose of this Jupyter Notebook\n",
    "    - To calculate the Elo Score of each mice after each interaction. The mice start off with an Elo score of 1000. Elo scores are calculated with the formula from here: https://www.omnicalculator.com/sports/elo . Then we will plot the change of elo score across all interactions. With the number of interactions on the X-Axis and the current Elo score on the Y. There will be a line for mice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import glob\n",
    "import ast\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "# Getting the path of the root directory so that we can import repo specific functions\n",
    "git_repo_object = git.Repo('.', search_parent_directories=True)\n",
    "git_repo_directory = git_repo_object.working_tree_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so that we can import functions\n",
    "sys.path.append(os.path.join(git_repo_directory, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nancy/projects/social_dominance_active_inference/src'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(git_repo_directory, \"src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elorating import calculation\n",
    "from elorating import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase size of plot in jupyter\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Inputs and Required data loading\n",
    "# input varaible names are in all caps snake case\n",
    "# Whenever an input changes or is used for processing \n",
    "# the vairables are all lower in snake case\n",
    "OUTPUT_DIR = r\"/root/work/\" # where data is saved should always be shown in the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Inputs and Required data loading\n",
    "# input varaible names are in all caps snake case\n",
    "# Whenever an input changes or is used for processing \n",
    "# the vairables are all lower in snake case\n",
    "OUTPUT_DIR = r\"./proc\" # where data is saved should always be shown in the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"../../data/pilot_3/Reward_Training_Weights_C57vsCD1_Pilot3.csv\"\n",
    "ELO_RATING_HISTORY_GLOB = glob.glob(\"./proc/elo_rating_spread_sheets/*/*elo-rating-history.csv\") \n",
    "\n",
    "PAIRED_RC = pd.read_csv(\"./proc/elo_rating_spread_sheets/reward_competition/reward_competition_grouped_by_pairs_cage_1_2_3_4_5_6_date_20221003_20221004.csv\", index_col=0)\n",
    "PAIRED_TT = pd.read_csv(\"./proc/elo_rating_spread_sheets/tube_test/tt_grouped_by_pairs_cage_cages-1-2-3-4-5-6_date_2022-09-06_2022-09-27.csv\", index_col=0)\n",
    "PAIRED_HCO = pd.read_csv(\"./proc/elo_rating_spread_sheets/home_cage_observation/hco_grouped_by_pairs_cage_cages-2-4-5_date_2022-09-07_2022-10-05.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELO_RATING_SPREADSHEET_OUTPUT_DIRECTORY = os.path.join(OUTPUT_DIR, \"elo_rating_spread_sheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(ELO_RATING_SPREADSHEET_OUTPUT_DIRECTORY, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3ee4891d43a4ac287413afc552ca289",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9ccbf6cc70fd4d379fa29317f733771f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe each output that the notebook creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc8e8920a6944918a15fac575cdf6e78",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Is it a plot or is it data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e639d4776a84aa9ac8ded2e14fa57db",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- How valuable is the output and why is it valuable or useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8999d19b6b7d4d63bc90f0b0bd9ab085",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b36cdf08567463082b005cb0dec684b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe what is done to the data here and how inputs are manipulated to generate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "89aaba237c644628b1b37604b75e7cb1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# As much code and as many cells as required\n",
    "# includes EDA and playing with data\n",
    "# GO HAM!\n",
    "\n",
    "# Ideally functions are defined here first and then data is processed using the functions\n",
    "\n",
    "# function names are short and in snake case all lowercase\n",
    "# a function name should be unique but does not have to describe the function\n",
    "# doc strings describe functions not function names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING_DF = pd.read_excel(RAW_DATA_FILE_PATH, sheet_name=INPUTTED_SHEET_NAME, header=ALL_HEADER_ROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_df = pd.read_excel(RAW_DATA_FILE_PATH, header=[0])\n",
    "weights_df = pd.read_csv(WEIGHTS_PATH, header=[0,1])\n",
    "weights_df = weights_df.set_index([('cage', 'cage'), ('id', 'id')])\n",
    "# Stacking the level 0 of the columns (Group1, Group2), \n",
    "# which will move it to the index, creating a new row for each unique value in level 0\n",
    "weights_df = weights_df.stack(level=1)\n",
    "weights_df = weights_df.reset_index()\n",
    "weights_df = weights_df.rename(columns={'level_2': 'date'})\n",
    "weights_df[\"date\"] = pd.to_datetime(weights_df[\"date\"], format='%m/%d/%Y')\n",
    "weights_df = weights_df.rename(columns={('cage', 'cage'): \"cage\", ('id', 'id'): \"id\"})\n",
    "weights_df = weights_df.sort_values(by=[\"date\", \"cage\", \"id\"])\n",
    "weights_df = weights_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cage', 'id', 'date', 'weight', 'percent_body_weight', 'amount_fed',\n",
       "       'Notes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_col = ['id', 'weight', 'percent_body_weight', 'amount_fed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_weights_df = weights_df.copy()\n",
    "subject_weights_df.columns = [\"subject_{}\".format(col) if col in weight_col else col for col in subject_weights_df.columns ]\n",
    "\n",
    "agent_weights_df = weights_df.copy()\n",
    "agent_weights_df.columns = [\"agent_{}\".format(col) if col in weight_col else col for col in agent_weights_df.columns  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cage</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>date</th>\n",
       "      <th>subject_weight</th>\n",
       "      <th>subject_percent_body_weight</th>\n",
       "      <th>subject_amount_fed</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>29.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cage  subject_id       date  subject_weight  subject_percent_body_weight  \\\n",
       "0     1         1.1 2022-09-18            26.0                          1.0   \n",
       "1     1         1.2 2022-09-18            25.2                          1.0   \n",
       "2     1         1.3 2022-09-18            24.8                          1.0   \n",
       "3     1         1.4 2022-09-18            24.7                          1.0   \n",
       "4     2         2.1 2022-09-18            29.5                          1.0   \n",
       "\n",
       "   subject_amount_fed Notes  \n",
       "0                 5.0   NaN  \n",
       "1                 5.0   NaN  \n",
       "2                 5.0   NaN  \n",
       "3                 5.0   NaN  \n",
       "4                 5.0   NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_weights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cage</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>date</th>\n",
       "      <th>agent_weight</th>\n",
       "      <th>agent_percent_body_weight</th>\n",
       "      <th>agent_amount_fed</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>29.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cage  agent_id       date  agent_weight  agent_percent_body_weight  \\\n",
       "0     1       1.1 2022-09-18          26.0                        1.0   \n",
       "1     1       1.2 2022-09-18          25.2                        1.0   \n",
       "2     1       1.3 2022-09-18          24.8                        1.0   \n",
       "3     1       1.4 2022-09-18          24.7                        1.0   \n",
       "4     2       2.1 2022-09-18          29.5                        1.0   \n",
       "\n",
       "   agent_amount_fed Notes  \n",
       "0               5.0   NaN  \n",
       "1               5.0   NaN  \n",
       "2               5.0   NaN  \n",
       "3               5.0   NaN  \n",
       "4               5.0   NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_weights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df = []\n",
    "for path in ELO_RATING_HISTORY_GLOB:\n",
    "    all_elo_score_history_df.append(pd.read_csv(path, index_col=0))\n",
    "all_elo_score_history_df = pd.concat(all_elo_score_history_df)\n",
    "all_elo_score_history_df[\"date\"] = pd.to_datetime(all_elo_score_history_df[\"date\"], format='%Y-%m-%d')\n",
    "all_elo_score_history_df = all_elo_score_history_df.sort_values(by=[\"date\", \"total_match_number\"])\n",
    "all_elo_score_history_df = all_elo_score_history_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_trial_number', 'total_match_number', 'subject_id', 'agent_id',\n",
       "       'original_elo_rating', 'updated_elo_rating', 'win_draw_loss',\n",
       "       'subject_ranking', 'agent_ranking', 'pairing_index', 'index', 'date',\n",
       "       'cage', 'box', 'match', 'tuple_animal_id', 'trial', 'winner',\n",
       "       'keep_row', 'match_is_tie', 'trial_number', 'loser',\n",
       "       'session_number_difference', 'strain', 'experiment_type', 'observer',\n",
       "       'notes', 'length_of_observations', 'cage_#', 'action', 'sheet_name',\n",
       "       'left_number_of_spots', 'right_number_of_spots',\n",
       "       'spot_number_difference', 'percent_difference', 'runner'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_elo_score_history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df = pd.merge(left=all_elo_score_history_df, right=subject_weights_df, left_on=[\"subject_id\", \"date\"], right_on=[\"subject_id\", \"date\"], how=\"left\")\n",
    "\n",
    "all_elo_score_history_df = pd.merge(left=all_elo_score_history_df, right=agent_weights_df, left_on=[\"agent_id\", \"date\"], right_on=[\"agent_id\", \"date\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_trial_number', 'total_match_number', 'subject_id', 'agent_id',\n",
       "       'original_elo_rating', 'updated_elo_rating', 'win_draw_loss',\n",
       "       'subject_ranking', 'agent_ranking', 'pairing_index', 'index', 'date',\n",
       "       'cage_x', 'box', 'match', 'tuple_animal_id', 'trial', 'winner',\n",
       "       'keep_row', 'match_is_tie', 'trial_number', 'loser',\n",
       "       'session_number_difference', 'strain', 'experiment_type', 'observer',\n",
       "       'notes', 'length_of_observations', 'cage_#', 'action', 'sheet_name',\n",
       "       'left_number_of_spots', 'right_number_of_spots',\n",
       "       'spot_number_difference', 'percent_difference', 'runner', 'cage_y',\n",
       "       'subject_weight', 'subject_percent_body_weight', 'subject_amount_fed',\n",
       "       'Notes_x', 'cage', 'agent_weight', 'agent_percent_body_weight',\n",
       "       'agent_amount_fed', 'Notes_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_elo_score_history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df = pd.merge(left=all_elo_score_history_df, right=PAIRED_HCO, left_on=\"tuple_animal_id\", right_on=\"hco_tuple_animal_id\", how=\"left\")\n",
    "all_elo_score_history_df = pd.merge(left=all_elo_score_history_df, right=PAIRED_RC, left_on=\"tuple_animal_id\", right_on=\"rc_tuple_animal_id\", how=\"left\")\n",
    "all_elo_score_history_df = pd.merge(left=all_elo_score_history_df, right=PAIRED_TT, left_on=\"tuple_animal_id\", right_on=\"tt_tuple_animal_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df = all_elo_score_history_df.drop(columns=[\"total_match_number\", \"total_trial_number\", \"index\", \"cage_x\", \"box\", \"match\", \"keep_row\", \"session_number_difference\", \"observer\", \"notes\", \"length_of_observations\", \"sheet_name\", \"runner\", \"cage_y\", \"cage\", \"Notes_y\", \"Notes_x\", 'processed_cage_number', 'match_is_tie', 'trial'], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'agent_id', 'original_elo_rating', 'updated_elo_rating',\n",
       "       'win_draw_loss', 'subject_ranking', 'agent_ranking', 'pairing_index',\n",
       "       'date', 'tuple_animal_id', 'winner', 'trial_number', 'loser', 'strain',\n",
       "       'experiment_type', 'cage_#', 'action', 'left_number_of_spots',\n",
       "       'right_number_of_spots', 'spot_number_difference', 'percent_difference',\n",
       "       'subject_weight', 'subject_percent_body_weight', 'subject_amount_fed',\n",
       "       'agent_weight', 'agent_percent_body_weight', 'agent_amount_fed',\n",
       "       'hco_tuple_animal_id', 'hco_winner', 'hco_loser', 'hco_averaged_winner',\n",
       "       'hco_averaged_loser', 'hco_averaged_winner_win_count',\n",
       "       'hco_averaged_loser_win_count', 'hco_count_difference',\n",
       "       'hco_match_count', 'hco_percent_win', 'hco_percentage_tie',\n",
       "       'rc_tuple_animal_id', 'rc_winner', 'rc_loser',\n",
       "       'rc_average_number_of_switches', 'rc_winner_no_ties',\n",
       "       'rc_loser_no_ties', 'rc_averaged_winner', 'rc_averaged_loser',\n",
       "       'rc_averaged_winner_win_count', 'rc_averaged_loser_win_count',\n",
       "       'rc_tie_count', 'rc_all_match_count_including_ties',\n",
       "       'rc_averaged_winner_win_count_minus_loser_win_count',\n",
       "       'rc_win_to_win_plus_lost_ratio', 'rc_win_to_all_ratio',\n",
       "       'rc_is_win_to_win_and_loss_ratio_tie', 'rc_tie_to_all_ratio',\n",
       "       'tt_tuple_animal_id', 'tt_winner', 'tt_loser', 'tt_averaged_winner',\n",
       "       'tt_averaged_loser', 'tt_averaged_winner_win_count',\n",
       "       'tt_averaged_loser_win_count', 'tt_count_difference', 'tt_match_count',\n",
       "       'tt_percent_win', 'tt_percentage_tie'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_elo_score_history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Specify the columns to move to the left\n",
    "cols_to_move = ['strain', 'experiment_type', 'date', 'cage_#', 'tuple_animal_id', 'subject_id', 'agent_id', \n",
    "                'winner', 'loser', 'win_draw_loss', 'original_elo_rating', 'updated_elo_rating', \n",
    "                'subject_ranking', 'agent_ranking',  'subject_weight', 'subject_percent_body_weight',\n",
    "                'subject_amount_fed', 'agent_weight', 'agent_percent_body_weight', 'agent_amount_fed']\n",
    "\n",
    "# Step 3: Get the list of other columns\n",
    "other_cols = [col for col in all_elo_score_history_df.columns if col not in cols_to_move]\n",
    "\n",
    "# Step 4: Concatenate the lists to get the new column order\n",
    "new_col_order = cols_to_move + other_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df = all_elo_score_history_df[new_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strain</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>date</th>\n",
       "      <th>cage_#</th>\n",
       "      <th>tuple_animal_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>winner</th>\n",
       "      <th>loser</th>\n",
       "      <th>win_draw_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>tt_winner</th>\n",
       "      <th>tt_loser</th>\n",
       "      <th>tt_averaged_winner</th>\n",
       "      <th>tt_averaged_loser</th>\n",
       "      <th>tt_averaged_winner_win_count</th>\n",
       "      <th>tt_averaged_loser_win_count</th>\n",
       "      <th>tt_count_difference</th>\n",
       "      <th>tt_match_count</th>\n",
       "      <th>tt_percent_win</th>\n",
       "      <th>tt_percentage_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C57</td>\n",
       "      <td>tube_test</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('1.1', '1.2')</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['1.1', '1.1', '1.1', '1.1', '1.1', '1.1', '1....</td>\n",
       "      <td>['1.2', '1.2', '1.2', '1.2', '1.2', '1.2', '1....</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C57</td>\n",
       "      <td>tube_test</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('1.1', '1.2')</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['1.1', '1.1', '1.1', '1.1', '1.1', '1.1', '1....</td>\n",
       "      <td>['1.2', '1.2', '1.2', '1.2', '1.2', '1.2', '1....</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C57</td>\n",
       "      <td>tube_test</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>('2.1', '2.2')</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['2.1', '2.1', '2.1', '2.1', '2.1', '2.1', '2....</td>\n",
       "      <td>['2.2', '2.2', '2.2', '2.2', '2.2', '2.2', '2....</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C57</td>\n",
       "      <td>tube_test</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>('2.1', '2.2')</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['2.1', '2.1', '2.1', '2.1', '2.1', '2.1', '2....</td>\n",
       "      <td>['2.2', '2.2', '2.2', '2.2', '2.2', '2.2', '2....</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C57</td>\n",
       "      <td>tube_test</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>('3.1', '3.2')</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['3.2', '3.2', '3.2', '3.2', '3.2', '3.2', '3....</td>\n",
       "      <td>['3.1', '3.1', '3.1', '3.1', '3.1', '3.1', '3....</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  strain experiment_type       date  cage_# tuple_animal_id  subject_id  \\\n",
       "0    C57       tube_test 2022-09-06     1.0  ('1.1', '1.2')         1.1   \n",
       "1    C57       tube_test 2022-09-06     1.0  ('1.1', '1.2')         1.2   \n",
       "2    C57       tube_test 2022-09-06     2.0  ('2.1', '2.2')         2.1   \n",
       "3    C57       tube_test 2022-09-06     2.0  ('2.1', '2.2')         2.2   \n",
       "4    C57       tube_test 2022-09-06     3.0  ('3.1', '3.2')         3.2   \n",
       "\n",
       "   agent_id  winner  loser  win_draw_loss  ...  \\\n",
       "0       1.2     1.1    1.2            1.0  ...   \n",
       "1       1.1     1.1    1.2            0.0  ...   \n",
       "2       2.2     2.1    2.2            1.0  ...   \n",
       "3       2.1     2.1    2.2            0.0  ...   \n",
       "4       3.1     3.2    3.1            1.0  ...   \n",
       "\n",
       "                                           tt_winner  \\\n",
       "0  ['1.1', '1.1', '1.1', '1.1', '1.1', '1.1', '1....   \n",
       "1  ['1.1', '1.1', '1.1', '1.1', '1.1', '1.1', '1....   \n",
       "2  ['2.1', '2.1', '2.1', '2.1', '2.1', '2.1', '2....   \n",
       "3  ['2.1', '2.1', '2.1', '2.1', '2.1', '2.1', '2....   \n",
       "4  ['3.2', '3.2', '3.2', '3.2', '3.2', '3.2', '3....   \n",
       "\n",
       "                                            tt_loser  tt_averaged_winner  \\\n",
       "0  ['1.2', '1.2', '1.2', '1.2', '1.2', '1.2', '1....                 1.1   \n",
       "1  ['1.2', '1.2', '1.2', '1.2', '1.2', '1.2', '1....                 1.1   \n",
       "2  ['2.2', '2.2', '2.2', '2.2', '2.2', '2.2', '2....                 2.1   \n",
       "3  ['2.2', '2.2', '2.2', '2.2', '2.2', '2.2', '2....                 2.1   \n",
       "4  ['3.1', '3.1', '3.1', '3.1', '3.1', '3.1', '3....                 3.2   \n",
       "\n",
       "   tt_averaged_loser  tt_averaged_winner_win_count  \\\n",
       "0                1.2                             8   \n",
       "1                1.2                             8   \n",
       "2                2.2                             7   \n",
       "3                2.2                             7   \n",
       "4                3.1                             8   \n",
       "\n",
       "   tt_averaged_loser_win_count  tt_count_difference  tt_match_count  \\\n",
       "0                            0                    8               8   \n",
       "1                            0                    8               8   \n",
       "2                            1                    6               8   \n",
       "3                            1                    6               8   \n",
       "4                            0                    8               8   \n",
       "\n",
       "   tt_percent_win  tt_percentage_tie  \n",
       "0           1.000              False  \n",
       "1           1.000              False  \n",
       "2           0.875              False  \n",
       "3           0.875              False  \n",
       "4           1.000              False  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_elo_score_history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['strain', 'experiment_type', 'date', 'cage_#', 'tuple_animal_id',\n",
       "       'subject_id', 'agent_id', 'winner', 'loser', 'win_draw_loss',\n",
       "       'original_elo_rating', 'updated_elo_rating', 'subject_ranking',\n",
       "       'agent_ranking', 'subject_weight', 'subject_percent_body_weight',\n",
       "       'subject_amount_fed', 'agent_weight', 'agent_percent_body_weight',\n",
       "       'agent_amount_fed', 'pairing_index', 'trial_number', 'action',\n",
       "       'left_number_of_spots', 'right_number_of_spots',\n",
       "       'spot_number_difference', 'percent_difference', 'hco_tuple_animal_id',\n",
       "       'hco_winner', 'hco_loser', 'hco_averaged_winner', 'hco_averaged_loser',\n",
       "       'hco_averaged_winner_win_count', 'hco_averaged_loser_win_count',\n",
       "       'hco_count_difference', 'hco_match_count', 'hco_percent_win',\n",
       "       'hco_percentage_tie', 'rc_tuple_animal_id', 'rc_winner', 'rc_loser',\n",
       "       'rc_average_number_of_switches', 'rc_winner_no_ties',\n",
       "       'rc_loser_no_ties', 'rc_averaged_winner', 'rc_averaged_loser',\n",
       "       'rc_averaged_winner_win_count', 'rc_averaged_loser_win_count',\n",
       "       'rc_tie_count', 'rc_all_match_count_including_ties',\n",
       "       'rc_averaged_winner_win_count_minus_loser_win_count',\n",
       "       'rc_win_to_win_plus_lost_ratio', 'rc_win_to_all_ratio',\n",
       "       'rc_is_win_to_win_and_loss_ratio_tie', 'rc_tie_to_all_ratio',\n",
       "       'tt_tuple_animal_id', 'tt_winner', 'tt_loser', 'tt_averaged_winner',\n",
       "       'tt_averaged_loser', 'tt_averaged_winner_win_count',\n",
       "       'tt_averaged_loser_win_count', 'tt_count_difference', 'tt_match_count',\n",
       "       'tt_percent_win', 'tt_percentage_tie'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_elo_score_history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elo_score_history_df.to_csv(\"./all_elo_score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the distance for reward port code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sheet names for the excel file\n",
    "xls = pd.ExcelFile(RAW_DATA_FILE_PATH)\n",
    "raw_data_sheet_names = xls.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through each sheet and creating a dataframe of it\n",
    "sheet_name_to_everything = defaultdict(dict)\n",
    "for sheet in raw_data_sheet_names:\n",
    "    sheet_name_to_everything[sheet][\"SCORING_DF\"] = pd.read_excel(RAW_DATA_FILE_PATH, sheet_name=sheet, header=ALL_HEADER_ROW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarizing the Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making all the column names lower case and removing any extra spaces in the beginning and at the end\n",
    "    - One dictionary per sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():\n",
    "    # Creating a dictionary that maps the original column name to the standarized one\n",
    "    column_name_to_standarized = defaultdict(dict)\n",
    "    for col in value[\"SCORING_DF\"]:\n",
    "        # Making the column name lower case and removing the spaces\n",
    "        column_name_to_standarized[col] = \"_\".join(str(col).lower().strip().split(\" \"))\n",
    "    value[\"column_name_to_standarized\"] = column_name_to_standarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"column_name_to_standarized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming all the columns to the lower case and space removed version\n",
    "for key, value in sheet_name_to_everything.items():\n",
    "    value[\"processed_behavior_recording_dataframe\"] = value[\"SCORING_DF\"].rename(columns=value[\"column_name_to_standarized\"])\n",
    "    value[\"processed_behavior_recording_dataframe\"][\"sheet_name\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"processed_behavior_recording_dataframe\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping All Rows without a Winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because we will be filling all empty cells with the previous value, we must remove all rows that don't have any data. This is usually cells that don't have any winners or losers filled in the row.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTE**: If there is a set column that is used for the winner and loser,  the rows up by session, then enter it in the cell below between the quotation marks. Default is `\"winner\"` and `\"loser\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():\n",
    "    # Dropping all rows that don't have any values in the winner column\n",
    "    value[\"processed_behavior_recording_dataframe\"] = value[\"processed_behavior_recording_dataframe\"].dropna(subset=WINNER_ID_COLUMN)\n",
    "    \n",
    "    # Dropping all rows that don't have any values in the loser column\n",
    "    value[\"processed_behavior_recording_dataframe\"] = value[\"processed_behavior_recording_dataframe\"].dropna(subset=LOSER_ID_COLUMN)\n",
    "    \n",
    "    # Getting all the floats from the strings, removing any spaces and other characters\n",
    "    value[\"processed_behavior_recording_dataframe\"][WINNER_ID_COLUMN] = value[\"processed_behavior_recording_dataframe\"][WINNER_ID_COLUMN].astype(str).apply(lambda x: re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", x)[0] if re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", x) else x)\n",
    "    \n",
    "    value[\"processed_behavior_recording_dataframe\"][LOSER_ID_COLUMN] = value[\"processed_behavior_recording_dataframe\"][LOSER_ID_COLUMN].astype(str).apply(lambda x: re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", x)[0] if re.findall(r\"[-+]?(?:\\d*\\.\\d+|\\d+)\", x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping only the rows with scorable actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specific for home cage observations. This protocol records the type of action that occured between an initiator and reciever. Only certain actions(usually fighting and chasing) are counted towards points for the ELO score. So we will remove all other rows that contain different actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():   \n",
    "    sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"][ALL_ACTION_COLUMN] = sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"][ALL_ACTION_COLUMN].apply(lambda x: x.strip())\n",
    "    \n",
    "    # Keeping all rows that have the inputted action\n",
    "    sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"] = sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"][sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"][ALL_ACTION_COLUMN].isin(ALL_SCORABLE_ACTION)]\n",
    "\n",
    "copy_of_sheet_name_to_everything = sheet_name_to_everything.copy()\n",
    "# Removing keys if the dataframe is empty    \n",
    "for key, value in copy_of_sheet_name_to_everything.items():\n",
    "    if sheet_name_to_everything[key][\"processed_behavior_recording_dataframe\"].empty:\n",
    "        sheet_name_to_everything.pop(key, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the session number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are adding the session number to all the trials. The session number is counting the number of recording sessions that have happened up until that trial. Usually, each session in the spreadsheet is divided up by a session's first row having the date filled in. So we will label a new session when a date is filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: If there is a set column that divides the rows up by session, then enter it in the cell below between the quotation marks. Default is `\"date\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():\n",
    "    current_processed_behavior_recording_dataframe = value[\"processed_behavior_recording_dataframe\"].copy()\n",
    "    value[\"processed_behavior_recording_dataframe\"][ALL_SESSION_DIVIDER_COLUMN] = value[\"processed_behavior_recording_dataframe\"][ALL_SESSION_DIVIDER_COLUMN].fillna(method='ffill')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"processed_behavior_recording_dataframe\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Session number differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the indexes of where each new session starts. So that we can add the session number to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():\n",
    "\n",
    "    # Filling all the empty cells with the value in the previous cell\n",
    "    value[\"processed_behavior_recording_dataframe\"] = value[\"processed_behavior_recording_dataframe\"].fillna(method='ffill')\n",
    "\n",
    "    value[\"processed_behavior_recording_dataframe\"][\"session_number_difference\"] = value[\"processed_behavior_recording_dataframe\"][ALL_SESSION_DIVIDER_COLUMN].astype('category').cat.codes.diff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"processed_behavior_recording_dataframe\"].head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"processed_behavior_recording_dataframe\"].tail(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the cage number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cage number is sometimes not consistent throughout a spreadsheet for the same cage. So we will try to standarize it into one value.\n",
    "    - **NOTE**: This assumes cage numbers are actual numbers. And not entirely consisting of letters. If that isn't the case, then you must edit this cell for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: If there is a column that has the cage number information, replace the `None` with the column name in quotation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change this when you have id to cage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sheet_name_to_everything['CAGE2'][\"processed_behavior_recording_dataframe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Elo rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation.calculate_elo_rating(subject_elo_rating=1000, agent_elo_rating=2000, score=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation.update_elo_rating(winner_id=\"A\", loser_id=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Elo rating for all the events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Going through each row or interaction and calculating the new Elo rating for the winner and loser. This will create a new dataframe based off of the processed behavioral recording dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: If there are a set of columns to keep, edit the cell below with the name of the columns each in quotation marks seperated by commas\n",
    "   - i.e. `['runner', 'date', 'match', 'winner', 'loser', 'notes', 'session_number',\n",
    "       'session_number_difference']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_to_keep = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sheet_name_to_everything.items():\n",
    "    # Calculating the Elo rating    \n",
    "    value[\"index_to_elo_rating_and_meta_data\"] = calculation.iterate_elo_rating_calculation_for_dataframe(dataframe=value[\"processed_behavior_recording_dataframe\"], winner_id_column=WINNER_ID_COLUMN, loser_id_column=LOSER_ID_COLUMN, additional_columns=value[\"processed_behavior_recording_dataframe\"].columns)\n",
    "\n",
    "     # Making a dataframe from the dictionary \n",
    "    value[\"elo_rating_dataframe\"] = pd.DataFrame.from_dict(value[\"index_to_elo_rating_and_meta_data\"], orient=\"index\")\n",
    "\n",
    "    value[\"elo_rating_dataframe\"][\"cage_#\"] = re.findall(r'\\d+', key)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"elo_rating_dataframe\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"elo_rating_dataframe\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value[\"elo_rating_dataframe\"].groupby(\"id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the Elo rating dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all the dataframes into one list\n",
    "all_sheet_elo_scord_dataframe_list = []\n",
    "for key, value in sheet_name_to_everything.items():    \n",
    "    all_sheet_elo_scord_dataframe_list.append(value[\"elo_rating_dataframe\"])\n",
    "\n",
    "# Combining all the dataframes into one\n",
    "all_sheet_elo_scord_dataframe_combined = pd.concat(all_sheet_elo_scord_dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the strain information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = set(all_sheet_elo_scord_dataframe_combined[\"id\"].unique()).union(set(all_sheet_elo_scord_dataframe_combined[\"agent_id\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: If there are strains that are associated to each cage, then create a dictionary of cage numbers to strains inside the `{}`\n",
    "- i.e. `CAGE_TO_STRAIN = {\"1\": \"C57\", \"2\": \"C57\", \"3\": \"C57\", \"4\": \"CD1\", \"5\": \"CD1\", \"6\": \"CD1\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined[\"strain\"] = all_sheet_elo_scord_dataframe_combined[\"cage_#\"].map(CAGE_TO_STRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the name of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the name of the experiment\n",
    "all_sheet_elo_scord_dataframe_combined[\"experiment_type\"] = PROTOCOL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking to see how many rows for each subject in each cage\n",
    "all_sheet_elo_scord_dataframe_combined.groupby(['id','cage_#']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe with only the final Elo rating for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see which cage and subject combination has more than one row\n",
    "all_sheet_elo_scord_dataframe_groupby = all_sheet_elo_scord_dataframe_combined.groupby(['id','cage_#']).size().reset_index()\n",
    "all_sheet_elo_scord_dataframe_groupby = all_sheet_elo_scord_dataframe_groupby[all_sheet_elo_scord_dataframe_groupby[0] >= 1]\n",
    "\n",
    "# Going through each combination and saving the combination to a dictionary\n",
    "index_to_id_and_processed_cage_number = defaultdict(dict)\n",
    "for index, row in all_sheet_elo_scord_dataframe_groupby.iterrows():\n",
    "    index_to_id_and_processed_cage_number[index]['id'] = row['id']\n",
    "    index_to_id_and_processed_cage_number[index]['cage_#'] = row['cage_#']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_to_id_and_processed_cage_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the final Elo rating for each cage and subject combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (key, value) in enumerate(index_to_id_and_processed_cage_number.items()):   \n",
    "    # The Elo rating dataframe for each cage and subject combination\n",
    "    per_subject_dataframe = all_sheet_elo_scord_dataframe_combined[(all_sheet_elo_scord_dataframe_combined[\"id\"] == value[\"id\"]) & (all_sheet_elo_scord_dataframe_combined[\"cage_#\"] == value[\"cage_#\"])]\n",
    "    # Getting the final Elo rating for each combination\n",
    "    # -1 Means that we're getting the data from the last row\n",
    "\n",
    "    index_to_id_and_processed_cage_number[index][\"final_elo_rating\"] = per_subject_dataframe.iloc[-1][\"updated_elo_rating\"]\n",
    "    \n",
    "    index_to_id_and_processed_cage_number[index][\"strain\"] = per_subject_dataframe.iloc[-1][\"strain\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_final_elo_rating_df = pd.DataFrame.from_dict(index_to_id_and_processed_cage_number, orient=\"index\")\n",
    "# Adding protocol name\n",
    "id_to_final_elo_rating_df[\"experiment_type\"] = PROTOCOL_NAME\n",
    "# Adding rank\n",
    "id_to_final_elo_rating_df[\"rank\"] = id_to_final_elo_rating_df.groupby(\"cage_#\")[\"final_elo_rating\"].rank(\"dense\", ascending=False)\n",
    "# Sorting by cage and then id\n",
    "id_to_final_elo_rating_df = id_to_final_elo_rating_df.sort_values(by=['cage_#', \"id\"], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_final_elo_rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_final_elo_rating_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making plots for all sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the dates the files were being recorded to use for the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sheet_elo_scord_dataframe_combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the earliest and the latest dates for all the recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_earlist_dates = []\n",
    "all_latest_dates = []\n",
    "for key, value in sheet_name_to_everything.items():\n",
    "    try:\n",
    "        # Getting all the earliest dates for each sheet\n",
    "        all_earlist_dates.append(value[\"elo_rating_dataframe\"][ALL_SESSION_DIVIDER_COLUMN].min())\n",
    "        all_latest_dates.append(value[\"elo_rating_dataframe\"][ALL_SESSION_DIVIDER_COLUMN].max())\n",
    "    except:\n",
    "        print(\"WARNING: {} does not have dates as columns\".format(key))\n",
    "        warnings.warn(\"Look at warning from above or below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[\"elo_rating_dataframe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Turning the Dates into a easier to read format\n",
    "    # Getting the 0th part of split to remove seconds\n",
    "    earliest_date = str(min(all_earlist_dates)).split()[0]\n",
    "    latest_date = str(max(all_latest_dates)).split()[0]\n",
    "    print(\"Earlist date: {}\".format(earliest_date))\n",
    "    print(\"Latest date: {}\".format(latest_date))\n",
    "except:\n",
    "    earliest_date = None\n",
    "    latest_date = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the cage numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cages_list = []\n",
    "# Creating a list of all the cage numbers\n",
    "for key, value in sheet_name_to_everything.items():\n",
    "    try:\n",
    "        for cage in value[\"elo_rating_dataframe\"][\"cage_#\"].unique():\n",
    "            all_cages_list.append(cage)\n",
    "    except:\n",
    "        print(\"WARNING: {} does not have cage number as columns\".format(key))\n",
    "        warnings.warn(\"Look at warning from above or below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_cages_string = \"-\".join(sorted([sheet.lower().strip(\"cage\").strip() for sheet in all_cages_list]))\n",
    "    all_cages_string = \"cages-{}\".format(all_cages_string)\n",
    "    print(\"String of cage names to use for file name: {}\".format(all_cages_string))\n",
    "except: \n",
    "    warnings.warn(\"WARNING: There are no cage numbers to make a title out of\")\n",
    "    all_cages_string = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating an output directory to save the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output_directory = os.path.join(\".\", \"proc\", \"plots\", \"{}_elo_rating\".format(PROTOCOL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(plot_output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTE**: Sometimes this cell needs to be run again to make sure the size is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the highest and lowest Elo rating for cutoffs of the Y-axis\n",
    "max_elo_rating = all_sheet_elo_scord_dataframe_combined[\"updated_elo_rating\"].max()\n",
    "min_elo_rating = all_sheet_elo_scord_dataframe_combined[\"updated_elo_rating\"].min()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (13.5,7.5)\n",
    "# Making a plot for each sheet\n",
    "for key, value in sheet_name_to_everything.items():\n",
    "    # Setting the size of the figure\n",
    "    plt.rcParams[\"figure.figsize\"] = (13.5,7.5)\n",
    "    print(key)\n",
    "    elo_rating_dataframe = value[\"elo_rating_dataframe\"]\n",
    "    # Using a new figure template for each sheet\n",
    "    fig, ax = plt.subplots()        \n",
    "        \n",
    "    try:\n",
    "        # Drawing vertical lines that represent when each session begins\n",
    "        # Based on when a row has a different session than the previous row\n",
    "        for index, row in elo_rating_dataframe[elo_rating_dataframe['session_number_difference'].astype(bool)].iterrows():\n",
    "            # Offsetting by 0.5 to avoid drawing the line on the dot\n",
    "            # Drawing the lines a little above the max and a little below the minimum\n",
    "            plt.vlines(x=[row[\"total_match_number\"] - 0.5], ymin=min_elo_rating-50, ymax=max_elo_rating+50, colors='black', linestyle='dashed')\n",
    "    except:\n",
    "        print(\"WARNING: {} does not have a column for session divider\".format(key))\n",
    "        warnings.warn(\"Look at warning from above or below\")\n",
    "            \n",
    "    # Drawing a line for each subject\n",
    "    for subject in sorted(elo_rating_dataframe[\"id\"].unique()):\n",
    "        # Getting all the rows with the current subject\n",
    "        subject_dataframe = elo_rating_dataframe[elo_rating_dataframe[\"id\"] == subject]\n",
    "        # Making the current match number the X-Axis\n",
    "        plt.plot(subject_dataframe[\"total_match_number\"], subject_dataframe[\"updated_elo_rating\"], '-o', label=subject)\n",
    "\n",
    "    # Labeling the X/Y Axis and the title\n",
    "    ax.set_xlabel(\"Trial Number\")\n",
    "    ax.set_ylabel(\"Elo rating\")\n",
    "    # Formattnig Cohort and Experiment Name so that it's more readable with spacing and capitalization\n",
    "    \n",
    "    ax.set_title(\"{} Elo Rating for {}\".format(PROTOCOL_NAME, key))\n",
    "    \n",
    "    # To show the legend\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    # Setting the values of the Y-axis\n",
    "    plt.ylim(min_elo_rating-50, max_elo_rating+50) \n",
    "    # Saving the plot\n",
    "    file_name_parts_separated = [PREFIX_NAME, key, earliest_date, latest_date]\n",
    "    file_name_parts_combined = \"_\".join([part for part in file_name_parts_separated if part])\n",
    "    \n",
    "    file_name_full = \"elo_rating_{}.png\".format(file_name_parts_combined)\n",
    "    # Removing all the spaces and replacing them with underscores\n",
    "    file_name_full = \"_\".join(file_name_full.split(\" \"))\n",
    "    plt.savefig(os.path.join(plot_output_directory, file_name_full))\n",
    "    # Showing the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a subfolder to put the Elo rating Spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_rating_spreadsheet_output_directory = os.path.join(\".\", \"proc\", \"elo_rating_spread_sheets\", \"{}\".format(PROTOCOL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_rating_spreadsheet_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(elo_rating_spreadsheet_output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the dataframes to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_parts_separated = [all_cages_string, PREFIX_NAME, earliest_date, latest_date]\n",
    "file_name_parts_combined = \"_\".join([part for part in file_name_parts_separated if part])\n",
    "\n",
    "file_name_full = \"{}_elo-rating-history.csv\".format(file_name_parts_combined)\n",
    "print(file_name_full)\n",
    "all_sheet_elo_scord_dataframe_combined.to_csv(os.path.join(elo_rating_spreadsheet_output_directory, file_name_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_full = \"{}_final-elo-rating.csv\".format(file_name_parts_combined)\n",
    "print(file_name_full)\n",
    "id_to_final_elo_rating_df.to_csv(os.path.join(elo_rating_spreadsheet_output_directory, file_name_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing which subject is the dominant or submissive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grouping all the rows with the same pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_list = []\n",
    "for key, value in sheet_name_to_everything.items():\n",
    "    all_processed_behavior_recording_list.append(value[\"processed_behavior_recording_dataframe\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining all the dataframes from all the cages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_df = pd.concat(all_processed_behavior_recording_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting a tuple of the animal IDs to be able to group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: This assumes that all the animal IDs are different across cages and that all IDs are numbers. i.e. there are no duplicate IDs in different cages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the animal IDs from the Match string\n",
    "# i.e. Getting all the floats and removing all spaces\n",
    "# Sorting so that the animal IDs are always in the same order\n",
    "all_processed_behavior_recording_df[\"animal_id\"] =  all_processed_behavior_recording_df.apply(lambda x: sorted([x[\"winner\"], x[\"loser\"]]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a tuple out of the list\n",
    "# Tuples are used because lists are mutable and can't be grouped with\n",
    "all_processed_behavior_recording_df[\"tuple_animal_id\"] = all_processed_behavior_recording_df[\"animal_id\"].apply(lambda x: tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing columns that would be unnecessary for the pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only the columns that we need\n",
    "all_processed_behavior_recording_df = all_processed_behavior_recording_df[['date', 'winner', 'loser', 'animal_id', 'tuple_animal_id', \"cage_#\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_behavior_recording_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the ID of the winner and the loser for each pair with each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wins_per_pair = all_processed_behavior_recording_df.groupby(\"tuple_animal_id\")['winner'].apply(list)\n",
    "all_loses_per_pair = all_processed_behavior_recording_df.groupby(\"tuple_animal_id\")['loser'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wins_per_pair[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making a dataframe of all the winner IDs and all the loser IDs for a given pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df = pd.concat([all_wins_per_pair, all_loses_per_pair], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df = all_competition_per_pair_df.rename(columns={k: PREFIX_NAME + \"_\" + k for k in all_competition_per_pair_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the cage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the cage number for each pair\n",
    "dropped_duplicate_all_processed_behavior_recording_df = all_processed_behavior_recording_df[[\"tuple_animal_id\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_duplicate_all_processed_behavior_recording_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the overall winner and loser. Also seeing if there is signficant difference in the number of wins to see if one is dominant over the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df[\"{}_averaged_winner\".format(PREFIX_NAME)] = all_competition_per_pair_df[\"{}_winner\".format(PREFIX_NAME)].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "all_competition_per_pair_df[\"{}_averaged_loser\".format(PREFIX_NAME)] = all_competition_per_pair_df[\"{}_loser\".format(PREFIX_NAME)].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "all_competition_per_pair_df[\"{}_winner_count\".format(PREFIX_NAME)] = all_competition_per_pair_df.apply(lambda x: x[\"{}_winner\".format(PREFIX_NAME)].count(x[\"{}_averaged_winner\".format(PREFIX_NAME)]), axis=1)\n",
    "all_competition_per_pair_df[\"{}_loser_count\".format(PREFIX_NAME)] = all_competition_per_pair_df.apply(lambda x: x[\"{}_winner\".format(PREFIX_NAME)].count(x[\"{}_averaged_loser\".format(PREFIX_NAME)]), axis=1)\n",
    "all_competition_per_pair_df[\"{}_count_difference\".format(PREFIX_NAME)] = all_competition_per_pair_df[\"{}_winner_count\".format(PREFIX_NAME)] - all_competition_per_pair_df[\"{}_loser_count\".format(PREFIX_NAME)]\n",
    "all_competition_per_pair_df[\"{}_match_count\".format(PREFIX_NAME)] = all_competition_per_pair_df[\"{}_winner\".format(PREFIX_NAME)].apply(lambda x: len(x))\n",
    "all_competition_per_pair_df[\"{}_percent_win\".format(PREFIX_NAME)] = all_competition_per_pair_df.apply(lambda x: x[\"{}_winner_count\".format(PREFIX_NAME)] / x[\"{}_match_count\".format(PREFIX_NAME)], axis=1)\n",
    "all_competition_per_pair_df[\"{}_percentage_tie\".format(PREFIX_NAME)] = all_competition_per_pair_df[\"{}_percent_win\".format(PREFIX_NAME)].apply(lambda x: True if x < 0.75 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the competiton pair results dataframe to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"{}_grouped_by_pairs_cage_{}_date_{}_{}.csv\".format(PREFIX_NAME, all_cages_string, earliest_date, latest_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_rating_spreadsheet_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competition_per_pair_df.to_csv(os.path.join(elo_rating_spreadsheet_output_directory, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a4490980-3f6a-4f44-80eb-ebd789a5b21f' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf8fe3695d074ee7887fdf6459cbf5ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
